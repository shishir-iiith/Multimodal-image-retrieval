{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868b463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc85e35-2d75-4922-9161-aa85cbbe2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2709a40b-5907-4513-9525-1364886f2b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe13b8fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] The directory name is invalid: 'D:/ImageCLEFphoto2008/annotations_complete_eng/.DS_Store/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23636/2860869583.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDATADIR\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtextpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] The directory name is invalid: 'D:/ImageCLEFphoto2008/annotations_complete_eng/.DS_Store/'"
     ]
    }
   ],
   "source": [
    "DATADIR = \"D:\\ImageCLEFphoto2008\\annotations_complete_eng\"\n",
    "#DATADIR = '/Users/apple/Documents/SMAI Assignments/flda/annotations_complete_eng'\n",
    "subfolders = os.listdir(DATADIR)\n",
    "#subfolders = subfolders[1:]\n",
    "texts_path = []\n",
    "texts = []\n",
    "path = ''\n",
    "for subfolder in subfolders:\n",
    "\n",
    "    path = DATADIR+\"/\"+subfolder+\"/\"\n",
    "    texts = os.listdir(path)\n",
    "\n",
    "    for textpath in texts:\n",
    "        texts_path.append(path+textpath)\n",
    "\n",
    "print(len(texts_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.shuffle(texts_path)  # Randomly shuffling the image names \n",
    "random_selection_text = np.random.choice(texts_path, 10000,replace=False)  # replace = False for unique paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ed1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selection_text[5687]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22987d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_descriptions = []\n",
    "\n",
    "annotation = None\n",
    "desc = None\n",
    "\n",
    "for i in tqdm(range(len(random_selection_text))):\n",
    "    with open(random_selection_text[i], 'r', encoding='latin-1') as file:\n",
    "        annotation = file.read()\n",
    "        desc = [string.strip() for string in re.findall(r'<DESCRIPTION>(.*?)</DESCRIPTION>', annotation, re.DOTALL)]\n",
    "        DATASET_descriptions.append(desc[0].replace(';',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_descriptions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd81e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DATASET_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d34a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DATASET_descriptions\n",
    " \n",
    "sentences = []\n",
    "word_set = set()\n",
    " \n",
    "for sent in data:\n",
    "    x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
    "    sentences.append(x)\n",
    "    for word in x:\n",
    "        word_set.add(word)\n",
    "#Set of vocab \n",
    "#word_set = set(word_set)\n",
    "#Total documents in our corpus\n",
    "total_documents = len(sentences)\n",
    " \n",
    "#Creating an index for each word in our vocab.\n",
    "index_dict = {} #Dictionary to store index for each word\n",
    "i = 0\n",
    "for word in word_set:\n",
    "    index_dict[word] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d75a63-4c12-4711-bc22-0795bb30b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1f686-dcd8-47df-95a5-75b16e7532b1",
   "metadata": {},
   "source": [
    "NOTE: Ganesh, i am calling **`sentences`** as **`sentences_tokenized`** from here as you already tokenized them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93056509-5386-4635-80cd-5d83967832d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateD_avg(sentences_tokenized):\n",
    "    average_size = 0\n",
    "    for sent in sentences_tokenized:\n",
    "        average_size+=len(sent)\n",
    "    average_size/= len(sentences_tokenized)\n",
    "    return average_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338b9e0-f539-4360-9448-b240e0574d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateFrequencyMatrix(sentences_tokenized):\n",
    "    # Tells how many times does a term appear in the given sentence( in our case description )\n",
    "    frequency_matrix = np.zeros((len(sentences_tokenized), len(word_set)))\n",
    "    \n",
    "    # Tells the no of documents in which the term appears\n",
    "    document_frequency = np.zeros(len(word_set))\n",
    "    \n",
    "    for i in range(len(sentences_tokenized)):\n",
    "        # Calculating the frequency of terms in sentence.\n",
    "        for word in sentences_tokenized[i]:\n",
    "            frequency_matrix[i, index_dict[word]]+=1\n",
    "        \n",
    "        # calculating the document frequency\n",
    "        document_frequency += (frequency_matrix[i]>0)*1\n",
    "    return frequency_matrix, document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acb00e-3892-40d2-a91f-666271374a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTermFrequencies(frequency_matrix, d_avg, sentences_tokenized):\n",
    "    term_frequency = np.zeros(frequency_matrix.shape)\n",
    "    for i in range(frequency_matrix.shape[0]):\n",
    "        term_frequency[i] = (frequency_matrix[i]*(1.2+1))/ (frequency_matrix[i] + 1.2*(1 - 0.75 + (0.75*(len(sentences_tokenized[i])/d_avg))))\n",
    "    return term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0883b7-2256-4d5e-ae38-0bcf7eb02307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateInverseDocFrequency(document_frequency, total_no_of_docs):\n",
    "    return np.log2( ((total_no_of_docs) - document_frequency + 0.5)/ (document_frequency + 0.5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297855d1-834e-4e4f-9786-05baead34896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTfIdf(term_frequency, inv_doc_frequency):\n",
    "    # copy the array so that we do not change term_frequency\n",
    "    tf_idf = np.copy(term_frequency)\n",
    "    for i in range(term_frequency.shape[0]):\n",
    "        tf_idf[i]*= inv_doc_frequency\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41657d36-5173-44ce-8589-68016178e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_matrix, document_frequency = calculateFrequencyMatrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000a4c4-5849-411e-9e31-6b1e11d126ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_avg = calculateD_avg(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908fb12-898d-4f71-8870-d85f16f494fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency = calculateTermFrequencies(frequency_matrix, d_avg, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822eec13-7b2d-45a7-b47c-83b035c33750",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_doc_frequency = calculateInverseDocFrequency(document_frequency, len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f569d-9db4-4d37-ae81-6cca52c70d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = calculateTfIdf(term_frequency, inverse_doc_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cec1f-7c0a-41d7-a767-da62244a0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidfvisual.pkl','wb') as output:\n",
    "    pickle.dump(tf_idf, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90d6ec-8544-447d-9c5e-ad5740668b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48615e-40bd-47a6-8b88-9fb470243f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_matrix[0,1270:1280] # position of 'a' in vocab is 1277, this will check if 'a' is repeated twice just like sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbc4d2-d079-4f7d-ad87-ef6613462b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4991e38-0b4a-46d3-b846-9ebedae55878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd207c7-3945-4901-9c98-9f41169023f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b12010",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To check whether the word is in the list or not \n",
    "ls = word_set\n",
    " \n",
    "if any(\"eleven\" in word for word in ls):\n",
    "    print('\\'eleven\\' is there inside the list!')\n",
    "else:\n",
    "    print('\\'eleven\\' is not there inside the list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a508b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a count dictionary\n",
    " \n",
    "def count_dict(sentences):\n",
    "    word_count = {}\n",
    "    for word in word_set:\n",
    "        word_count[word] = 0\n",
    "        for sent in sentences:\n",
    "            if word in sent:\n",
    "                word_count[word] += 1\n",
    "    return word_count\n",
    " \n",
    "word_count = count_dict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac1b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidfscorecalc:\n",
    "    def __init__(self):\n",
    "        # tf-idf scores of images\n",
    "        self.tf_idf = []\n",
    "        # Inverse document frequency of images\n",
    "        self.inv_doc_freq = []\n",
    "        self.k = 0\n",
    "        \n",
    "        \n",
    "    def computeTfIdfScoredMSTD(self, texts):\n",
    "        # contains the term frequency of each text\n",
    "        document_frequency = np.zeros(self.k)\n",
    "\n",
    "        tf_idf = []\n",
    "        print(\"Calculating tf-idf scores ...\")\n",
    "        \n",
    "        for i in tqdm(range(len(texts))):\n",
    "            \n",
    "            term_frequency = np.zeros(self.k)\n",
    "            # Return labels\n",
    "            visual_terms = self.getVisualTermsOfImage(texts[i])\n",
    "            \n",
    "            # Calculate the histogram\n",
    "            for i in range(self.k):\n",
    "                term_frequency[visual_terms[i]]+=1 \n",
    "            # Add the elements that appear in this document\n",
    "            document_frequency += (term_frequency>0)*1\n",
    "            \n",
    "            # K is 1.2 in Okapi formula. b is not necessary as the average size of documents is same as document size for all images\n",
    "            term_frequency = (term_frequency*(1.2+1))/(term_frequency+1.2)\n",
    "            tf_idf.append(term_frequency)\n",
    "        \n",
    "        inverse_document_frequency = np.log2((len(images)-document_frequency + 0.5)/(document_frequency+0.5))\n",
    "        for i in range(len(images)):\n",
    "            tf_idf[i] *= inverse_document_frequency\n",
    "            \n",
    "        self.tf_idf = tf_idf\n",
    "        self.inv_doc_freq = inverse_document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency\n",
    "def termfreq(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse Document Frequency\n",
    " \n",
    "def inverse_doc_freq(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(total_documents/word_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(sentence):\n",
    "    tf_idf_vec = np.zeros((len(word_set),))\n",
    "    for word in sentence:\n",
    "        tf = termfreq(sentence,word)\n",
    "        idf = inverse_doc_freq(word)\n",
    "         \n",
    "        value = tf*idf\n",
    "        tf_idf_vec[index_dict[word]] = value \n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sent):\n",
    "    count_dict = defaultdict(int)\n",
    "    vec = np.zeros(len_vector)\n",
    "    for item in sent:\n",
    "        count_dict[item] += 1\n",
    "    for key,item in count_dict.items():\n",
    "        vec[index_dict[key]] = item\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39044f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector = bag_of_words(sentences[109])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fdf90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "var = None\n",
    "with open(\"tfidftextual.pkl\", \"rb\") as file:\n",
    "    pickle.load(file)\n",
    "    \n",
    "var.tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bacd5b-0b2c-45ba-8d26-3e0ec7bc2e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
